{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f143b0",
   "metadata": {},
   "source": [
    "### Inference is based on  the following kaggle kernels: \n",
    "https://www.kaggle.com/akuritsyn/two-longformers-are-better-than-1\n",
    "and\n",
    "https://www.kaggle.com/hengck23/1-birdformer-1-longformer-one-fold/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef033057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../tez/\")\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "import tez\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from joblib import Parallel, delayed\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "\n",
    "# added for metrics computation\n",
    "sys.path.append(\"../src/\")\n",
    "from utils import FeedbackDatasetValid, score_feedback_comp, \\\n",
    "    score_feedback_comp_micro, calc_overlap, prepare_training_data, _prepare_training_data_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)\n",
    "\n",
    "n_models = 5\n",
    "\n",
    "num_discourse_marker = 15\n",
    "\n",
    "class mod_args:\n",
    "    input_path = \"../data/\"\n",
    "    input = \"../data/\"\n",
    "    model_def = [\n",
    "        \"../model/microsoft/deberta-large\",\n",
    "        \"../model/microsoft/deberta-large\",\n",
    "        \"../model/allenai/longformer-large-4096/\",\n",
    "        \"../model/microsoft/deberta-v3-large\",\n",
    "        \"../model/microsoft/deberta-v3-large\",\n",
    "    ]\n",
    "    model_path = [\n",
    "        \"../output/deberta-large1-1024\",\n",
    "        \"../output/deberta-large2-1024\",\n",
    "        \"../output/fb-longformer-large-1536/\",\n",
    "        \"../output/deberta-v3-large1-1024\",\n",
    "        \"../output/deberta-v3-large2-1024\",\n",
    "    ]\n",
    "    folds = [[1], [1], [1], [1], [1]]\n",
    "    output = \".\"\n",
    "    batch_size = [1, 1, 4, 4, 4]\n",
    "    max_len = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id_map = {\n",
    "    \"B-Lead\": 0,\n",
    "    \"I-Lead\": 1,\n",
    "    \"B-Position\": 2,\n",
    "    \"I-Position\": 3,\n",
    "    \"B-Evidence\": 4,\n",
    "    \"I-Evidence\": 5,\n",
    "    \"B-Claim\": 6,\n",
    "    \"I-Claim\": 7,\n",
    "    \"B-Concluding Statement\": 8,\n",
    "    \"I-Concluding Statement\": 9,\n",
    "    \"B-Counterclaim\": 10,\n",
    "    \"I-Counterclaim\": 11,\n",
    "    \"B-Rebuttal\": 12,\n",
    "    \"I-Rebuttal\": 13,\n",
    "    \"O\": 14,\n",
    "    \"PAD\": -100,\n",
    "}\n",
    "\n",
    "id_target_map = {v: k for k, v in target_id_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30762e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackDataset:\n",
    "    def __init__(self, samples, max_len, tokenizer):\n",
    "        self.samples = samples\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.length = len(samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.samples[idx][\"input_ids\"]\n",
    "        input_ids = [self.tokenizer.cls_token_id] + input_ids\n",
    "\n",
    "        if len(input_ids) > self.max_len - 1:\n",
    "            input_ids = input_ids[: self.max_len - 1]\n",
    "\n",
    "        # add end token id to the input_ids\n",
    "        input_ids = input_ids + [self.tokenizer.sep_token_id]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        return {\n",
    "            \"ids\": input_ids,\n",
    "            \"mask\": attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n",
    "        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n",
    "            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n",
    "        else:\n",
    "            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n",
    "            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n",
    "        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab75f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_test_data_helper(args, tokenizer, ids):\n",
    "    test_samples = []\n",
    "    for idx in ids:\n",
    "        filename = os.path.join(args.input_path, \"test\", idx + \".txt\")\n",
    "        with open(filename, \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        input_ids = encoded_text[\"input_ids\"]\n",
    "        offset_mapping = encoded_text[\"offset_mapping\"]\n",
    "\n",
    "        sample = {\n",
    "            \"id\": idx,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"text\": text,\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "        }\n",
    "\n",
    "        test_samples.append(sample)\n",
    "    return test_samples\n",
    "\n",
    "\n",
    "def prepare_test_data(df, tokenizer, args):\n",
    "    test_samples = []\n",
    "    ids = df[\"id\"].unique()\n",
    "    ids_splits = np.array_split(ids, 4)\n",
    "\n",
    "    results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n",
    "        delayed(_prepare_test_data_helper)(args, tokenizer, idx) for idx in ids_splits\n",
    "    )\n",
    "    for result in results:\n",
    "        test_samples.extend(result)\n",
    "\n",
    "    return test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd1236",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackModel(tez.Model):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "        hidden_dropout_prob: float = 0.15\n",
    "        layer_norm_eps: float = 1e-7\n",
    "        config.update(\n",
    "            {\n",
    "                \"output_hidden_states\": True,\n",
    "                \"hidden_dropout_prob\": hidden_dropout_prob,\n",
    "                \"layer_norm_eps\": layer_norm_eps,\n",
    "                \"add_pooling_layer\": False,\n",
    "            }\n",
    "        )\n",
    "        self.transformer = AutoModel.from_config(config)\n",
    "        self.output = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        transformer_out = self.transformer(ids, mask)\n",
    "        sequence_output = transformer_out.last_hidden_state\n",
    "        logits = self.output(sequence_output)\n",
    "        logits = torch.softmax(logits, dim=-1)\n",
    "        return logits, 0, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a12f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ensembling approach from https://www.kaggle.com/hengck23/1-birdformer-1-longformer-one-fold/\n",
    "\n",
    "FOLD =  mod_args.folds[0][0]\n",
    "print(FOLD)\n",
    "\n",
    "df = pd.read_csv(os.path.join(mod_args.input_path, \"train_folds.csv\"))\n",
    "valid_df = df[df[\"kfold\"] == FOLD].reset_index(drop=True)\n",
    "\n",
    "result_nn = []\n",
    "\n",
    "for i in range(n_models):\n",
    "    \n",
    "    if mod_args.model_def[i] == \"../model/microsoft/deberta-v3-large\":\n",
    "        tokenizer = DebertaV2TokenizerFast.from_pretrained(mod_args.model_def[i])\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(mod_args.model_def[i])\n",
    "        \n",
    "    collate = Collate(tokenizer)\n",
    "\n",
    "    test_samples = prepare_training_data(valid_df, tokenizer, mod_args, num_jobs=4)\n",
    "    test_samples.sort(key=lambda d: d[\"text_len\"], reverse=False)\n",
    "    test_dataset = FeedbackDatasetValid(test_samples, mod_args.max_len, tokenizer)\n",
    "\n",
    "    # print(mod_args.model_def[i])\n",
    "    model = FeedbackModel(model_name=mod_args.model_def[i], num_labels=len(target_id_map) - 1)\n",
    "    \n",
    "    n_folds = len(mod_args.folds[i])\n",
    "    model_prob = []\n",
    "    first_fold = mod_args.folds[i][0]\n",
    "    \n",
    "    for _fold in mod_args.folds[i]:\n",
    "        # print(mod_args.model_path[i])\n",
    "        model.load(os.path.join(mod_args.model_path[i], f\"model_{_fold}.bin\"), weights_only=True)\n",
    "        model.eval()\n",
    "\n",
    "        preds_iter = model.predict(test_dataset, batch_size=mod_args.batch_size[i], n_jobs=-1, collate_fn=collate,)\n",
    "\n",
    "        current_idx = 0\n",
    "        for preds in preds_iter:\n",
    "            preds = preds.astype(np.float16)\n",
    "            preds = preds / n_folds\n",
    "            \n",
    "            if _fold == first_fold:\n",
    "                model_prob.append(preds)\n",
    "            else:\n",
    "                model_prob[current_idx] += preds\n",
    "                current_idx += 1\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "            \n",
    "    model_prob1 = [item for sublist in model_prob for item in sublist]\n",
    "    for j in range(len(test_samples)):\n",
    "        test_samples[j][\"probability\"] = model_prob1[j]\n",
    "\n",
    "    result_nn.append(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6319408",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_threshold = {\n",
    "    'Lead'                : 6,\n",
    "    'Position'            : 4,\n",
    "    'Claim'               : 3,\n",
    "    'Counterclaim'        : 7,\n",
    "    'Rebuttal'            : 4,\n",
    "    'Evidence'            : 14,\n",
    "    'Concluding Statement': 7,\n",
    "}\n",
    "\n",
    "probability_threshold = {\n",
    "    'Lead'                : 0.55,\n",
    "    'Position'            : 0.55,\n",
    "    'Claim'               : 0.50,\n",
    "    'Counterclaim'        : 0.50,\n",
    "    'Rebuttal'            : 0.55,\n",
    "    'Evidence'            : 0.60,\n",
    "    'Concluding Statement': 0.60,\n",
    "}\n",
    "\n",
    "def do_threshold(submit_df, use=['length','probability']):\n",
    "    df = submit_df.copy()\n",
    "    df = df.fillna('')\n",
    "    \n",
    "    if 'probability' in use:\n",
    "        df['s'] = df.score.apply(lambda x: np.mean(eval(x)))\n",
    "        for key, value in probability_threshold.items():\n",
    "            index = df.loc[df['class'] == key].query('s<%f'%value).index\n",
    "            df.drop(index, inplace=True)\n",
    "\n",
    "    if 'length' in use:\n",
    "        df['l'] = df.predictionstring.apply(lambda x: len(x.split()))\n",
    "        for key, value in length_threshold.items():\n",
    "            index = df.loc[df['class'] == key].query('l<%d'%value).index\n",
    "            df.drop(index, inplace=True)\n",
    "\n",
    "    df = df[['id', 'class', 'predictionstring']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e2116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word(text):\n",
    "    word = text.split()\n",
    "    word_offset = []\n",
    "\n",
    "    start = 0\n",
    "    for w in word:\n",
    "        r = text[start:].find(w)\n",
    "\n",
    "        if r == -1:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            start = start + r\n",
    "            end = start + len(w)\n",
    "            word_offset.append((start, end))\n",
    "\n",
    "        start = end\n",
    "\n",
    "    return word, word_offset\n",
    "\n",
    "def word_probability_to_predict_df(text_to_word_probability, id):\n",
    "    len_word = len(text_to_word_probability)\n",
    "    word_predict = text_to_word_probability.argmax(-1)\n",
    "    word_score   = text_to_word_probability.max(-1)\n",
    "    predict_df = []\n",
    "\n",
    "    t = 0\n",
    "    while 1:\n",
    "        if word_predict[t] not in [\n",
    "            target_id_map['O'],\n",
    "            target_id_map['PAD'],\n",
    "        ]:\n",
    "            start = t\n",
    "            b_marker_label = word_predict[t]\n",
    "        else:\n",
    "            t = t+1\n",
    "            if t == len_word-1: break\n",
    "            continue\n",
    "\n",
    "        t = t+1\n",
    "        if t== len_word-1: break\n",
    "\n",
    "        #----\n",
    "        if   id_target_map[b_marker_label][0]=='B':\n",
    "            i_marker_label = b_marker_label+1\n",
    "        elif id_target_map[b_marker_label][0]=='I':\n",
    "            i_marker_label = b_marker_label\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        while 1:\n",
    "            if (word_predict[t] != i_marker_label) or (t ==len_word-1):\n",
    "                end = t\n",
    "                prediction_string = ' '.join([str(i) for i in range(start,end)]) #np.arange(start,end).tolist()\n",
    "                discourse_type = id_target_map[b_marker_label][2:]\n",
    "                discourse_score = word_score[start:end].tolist()\n",
    "                predict_df.append((id, discourse_type, prediction_string, str(discourse_score)))\n",
    "                #print(predict_df[-1])\n",
    "                break\n",
    "            else:\n",
    "                t = t+1\n",
    "                continue\n",
    "        if t== len_word-1: break\n",
    "\n",
    "    predict_df = pd.DataFrame(predict_df, columns=['id', 'class', 'predictionstring', 'score'])\n",
    "    return predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/kaggleqrdl/tensorflow-longformer-ner-postprocessing.\n",
    "def jn(pst, start, end):\n",
    "    return \" \".join([str(x) for x in pst[start:end]])\n",
    "\n",
    "\n",
    "def link_evidence(oof):\n",
    "    thresh = 1\n",
    "    idu = oof['id'].unique()\n",
    "    idc = idu[1]\n",
    "    eoof = oof[oof['class'] == \"Evidence\"]\n",
    "    neoof = oof[oof['class'] != \"Evidence\"]\n",
    "    for thresh2 in range(26, 27, 1):\n",
    "        retval = []\n",
    "        for idv in idu:\n",
    "            for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n",
    "                   'Counterclaim', 'Rebuttal']:\n",
    "                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n",
    "                if len(q) == 0:\n",
    "                    continue\n",
    "                pst = []\n",
    "                for i,r in q.iterrows():\n",
    "                    pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n",
    "                start = 1\n",
    "                end = 1\n",
    "                for i in range(2,len(pst)):\n",
    "                    cur = pst[i]\n",
    "                    end = i\n",
    "                    #if pst[start] == 205:\n",
    "                    #   print(cur, pst[start], cur - pst[start])\n",
    "                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n",
    "                        retval.append((idv, c, jn(pst, start, end)))\n",
    "                        start = i + 1\n",
    "                v = (idv, c, jn(pst, start, end+1))\n",
    "                #print(v)\n",
    "                retval.append(v)\n",
    "        roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n",
    "        roof = roof.merge(neoof, how='outer')\n",
    "        return roof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b8005",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_valid = len(result_nn[0])\n",
    "\n",
    "submit_df = []\n",
    "for i in range(num_valid):\n",
    "    \n",
    "    text_id = result_nn[0][i][\"id\"]\n",
    "    text = result_nn[0][i][\"text\"]\n",
    "    word, word_offset = text_to_word(text)\n",
    "        \n",
    "    # --- ensemble\n",
    "    token_to_text_probability = np.full((len(text), num_discourse_marker), 0, np.float32)\n",
    "    for j in range(n_models):\n",
    "        p = result_nn[j][i][\"probability\"][1:] # [:-1]\n",
    "\n",
    "        for t, (start, end) in enumerate(result_nn[j][i][\"offset_mapping\"]):\n",
    "            if t == mod_args.max_len - 1:\n",
    "                break\n",
    "            token_to_text_probability[start:end] += p[t]\n",
    "    token_to_text_probability = token_to_text_probability / n_models\n",
    "    # -------------\n",
    "    \n",
    "    text_to_word_probability = np.full((len(word), num_discourse_marker), 0, np.float32)\n",
    "    for t, (start, end) in enumerate(word_offset):\n",
    "        text_to_word_probability[t] = token_to_text_probability[start:end].mean(0)\n",
    "\n",
    "    predict_df = word_probability_to_predict_df(text_to_word_probability, text_id)\n",
    "    submit_df.append(predict_df)\n",
    "    \n",
    "    # if i % 300 == 0: print(i, text_id, len(text), len(word))\n",
    "    \n",
    "submit_df = pd.concat(submit_df).reset_index(drop=True)\n",
    "\n",
    "submit_df = do_threshold(submit_df, use=['length', 'probability'])\n",
    "submit_df = link_evidence(submit_df)\n",
    "# submit_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FOLD : {FOLD}\")\n",
    "scr = score_feedback_comp(submit_df, valid_df, return_class_scores=True)\n",
    "print(scr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
